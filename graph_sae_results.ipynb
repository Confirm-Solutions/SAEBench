{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "from typing import Optional\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from graphing_utils import plot_2var_graph, plot_3var_graph, plot_interactive_3var_graph, plot_steps_vs_average_diff\n",
    "from formatting_utils import get_sparsity_penalty, ae_config_results, add_custom_metric_results, filter_by_l0_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_name = \"images\"\n",
    "\n",
    "if not os.path.exists(image_folder_name):\n",
    "    os.makedirs(image_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_name = \"sae_bench_pythia70m_sweep_topk_ctx128_0730\"\n",
    "\n",
    "folder_path = \"sparse_probing/src/sparse_probing_results\"\n",
    "filename = f\"example_results_{release_name}_eval_results.json\"\n",
    "\n",
    "filepath = os.path.join(folder_path, filename)\n",
    "\n",
    "with open(filepath, \"r\") as f:\n",
    "    eval_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_results.keys())\n",
    "print(eval_results[\"custom_eval_results\"].keys())\n",
    "print(eval_results[\"custom_eval_results\"][\"pythia70m_sweep_topk_ctx128_0730/resid_post_layer_4/trainer_0\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_data_filename = f\"sae_bench_data/{release_name}_data.json\"\n",
    "\n",
    "with open(sae_data_filename, \"r\") as f:\n",
    "    sae_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sae_data.keys())\n",
    "print(sae_data[\"basic_eval_results\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_results = {}\n",
    "k= 100\n",
    "\n",
    "custom_metric = f'sae_top_{k}_test_accuracy'\n",
    "\n",
    "for sae_name in eval_results['custom_eval_results']:\n",
    "    plotting_results[sae_name] = {}\n",
    "\n",
    "    plotting_results[sae_name]['trainer_class'] = sae_data['sae_config_dictionary_learning'][sae_name][\"trainer\"][\"trainer_class\"]\n",
    "    plotting_results[sae_name]['l0'] = sae_data['basic_eval_results'][sae_name]['l0']\n",
    "    plotting_results[sae_name]['frac_recovered'] = sae_data['basic_eval_results'][sae_name]['frac_recovered']\n",
    "\n",
    "    plotting_results[sae_name][custom_metric] = eval_results['custom_eval_results'][sae_name][custom_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metric_name = f\"{k}-Sparse Probe Accuracy\"\n",
    "title = f\"L0 vs Loss Recovered vs {custom_metric_name}\"\n",
    "image_base_name = os.path.join(image_folder_name, custom_metric)\n",
    "\n",
    "plot_3var_graph(\n",
    "    plotting_results,\n",
    "    title,\n",
    "    custom_metric,\n",
    "    colorbar_label=\"Custom Metric\",\n",
    "    output_filename=f\"{image_base_name}_3var.png\",\n",
    ")\n",
    "plot_2var_graph(\n",
    "    plotting_results,\n",
    "    custom_metric,\n",
    "    title=title,\n",
    "    output_filename=f\"{image_base_name}_2var.png\",\n",
    ")\n",
    "# plot_interactive_3var_graph(plotting_results, custom_metric)\n",
    "\n",
    "# At this point, if there's any additional .json files located alongside the ae.pt and eval_results.json\n",
    "# You can easily adapt them to be included in the plotting_results dictionary by using something similar to add_ae_config_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fix below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(\n",
    "    plotting_results: dict[str, dict[str, float]],\n",
    "    metric_names: list[str],\n",
    "    ae_names: Optional[list[str]] = None,\n",
    "    title: str = \"Metric Correlation Heatmap\",\n",
    "    output_filename: str = None,\n",
    "    figsize: tuple = (12, 10),\n",
    "    cmap: str = \"coolwarm\",\n",
    "    annot: bool = True,\n",
    "):\n",
    "    # If ae_names is not provided, use all ae_names from plotting_results\n",
    "    if ae_names is None:\n",
    "        ae_names = list(plotting_results.keys())\n",
    "\n",
    "    # If metric_names is not provided, use all metric names from the first ae_name\n",
    "    # if metric_names is None:\n",
    "    #     metric_names = list(plotting_results[ae_names[0]].keys())\n",
    "\n",
    "    # Create a DataFrame from the plotting_results\n",
    "    data = []\n",
    "    for ae in ae_names:\n",
    "        row = [plotting_results[ae].get(metric, np.nan) for metric in metric_names]\n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data, index=ae_names, columns=metric_names)\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(corr_matrix, annot=annot, cmap=cmap, vmin=-1, vmax=1, center=0)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if output_filename is provided\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "metric_keys = [\n",
    "    \"l0\",\n",
    "    \"frac_recovered\",\n",
    "    \"tpp_attrib_threshold_10_total_metric\",\n",
    "    \"tpp_attrib_threshold_50_total_metric\",\n",
    "    \"tpp_attrib_threshold_500_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_10_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_50_total_metric\",\n",
    "]\n",
    "\n",
    "plot_correlation_heatmap(plotting_results, metric_names=metric_keys, ae_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def plot_metric_scatter(\n",
    "    plotting_results: dict[str, dict[str, float]],\n",
    "    metric_x: str,\n",
    "    metric_y: str,\n",
    "    x_label: Optional[str] = None,\n",
    "    y_label: Optional[str] = None,\n",
    "    ae_names: Optional[list[str]] = None,\n",
    "    title: str = \"Metric Comparison Scatter Plot\",\n",
    "    output_filename: Optional[str] = None,\n",
    "    figsize: tuple = (10, 8),\n",
    "):\n",
    "    # If ae_names is not provided, use all ae_names from plotting_results\n",
    "    if ae_names is None:\n",
    "        ae_names = list(plotting_results.keys())\n",
    "\n",
    "    # Extract x and y values for the specified metrics\n",
    "    x_values = [plotting_results[ae].get(metric_x, float(\"nan\")) for ae in ae_names]\n",
    "    y_values = [plotting_results[ae].get(metric_y, float(\"nan\")) for ae in ae_names]\n",
    "\n",
    "    # Remove any NaN values\n",
    "    valid_data = [\n",
    "        (x, y, ae)\n",
    "        for x, y, ae in zip(x_values, y_values, ae_names)\n",
    "        if not (np.isnan(x) or np.isnan(y))\n",
    "    ]\n",
    "    if not valid_data:\n",
    "        print(\"No valid data points after removing NaN values.\")\n",
    "        return\n",
    "\n",
    "    x_values, y_values, valid_ae_names = zip(*valid_data)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    x_values = np.array(x_values)\n",
    "    y_values = np.array(y_values)\n",
    "\n",
    "    # Calculate correlation coefficients\n",
    "    r, p_value = stats.pearsonr(x_values, y_values)\n",
    "    r_squared = r**2\n",
    "\n",
    "    # Create the scatter plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    scatter = sns.scatterplot(x=x_values, y=y_values, label=\"SAE\", color=\"blue\")\n",
    "\n",
    "    if x_label is None:\n",
    "        x_label = metric_x\n",
    "    if y_label is None:\n",
    "        y_label = metric_y\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "\n",
    "    # Add a trend line\n",
    "    sns.regplot(x=x_values, y=y_values, scatter=False, color=\"red\", label=f\"r = {r:.4f}\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if output_filename is provided\n",
    "    if output_filename:\n",
    "        plt.savefig(output_filename, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print correlation coefficients\n",
    "    print(f\"Pearson correlation coefficient (r): {r:.4f}\")\n",
    "    print(f\"Coefficient of determination (rÂ²): {r_squared:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# plot_metric_scatter(plotting_results, metric_x=\"l0\", metric_y=\"frac_recovered\", title=\"L0 vs Fraction Recovered\")\n",
    "\n",
    "metric1 = f\"tpp_auto_interp_threshold_{threshold}_total_metric\"\n",
    "metric2 = f\"tpp_attrib_threshold_{threshold}_total_metric\"\n",
    "title = f\"\"\n",
    "\n",
    "x_label = \"TPP with LLM Judge\"\n",
    "y_label = \"TPP without LLM Judge\"\n",
    "output_filename = os.path.join(image_folder_name, f\"tpp_comparison_{threshold}_{model_name}.png\")\n",
    "\n",
    "plot_metric_scatter(\n",
    "    plotting_results,\n",
    "    metric_x=metric1,\n",
    "    metric_y=metric2,\n",
    "    title=title,\n",
    "    x_label=x_label,\n",
    "    y_label=y_label,\n",
    "    output_filename=output_filename,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_key = next(iter(plotting_results.keys()))\n",
    "print(plotting_results[first_key].keys())\n",
    "\n",
    "metric_keys = [\n",
    "    \"l0\",\n",
    "    \"frac_recovered\",\n",
    "    \"tpp_attrib_threshold_20_total_metric\",\n",
    "    \"tpp_attrib_threshold_50_total_metric\",\n",
    "    \"tpp_attrib_threshold_500_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_20_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_50_total_metric\",\n",
    "    \"scr_bias_shift_dir2_threshold_20\",\n",
    "    \"scr_bias_shift_dir2_threshold_50\",\n",
    "    \"scr_bias_shift_dir1_threshold_20\",\n",
    "    \"scr_bias_shift_dir1_threshold_50\",\n",
    "    \"scr_attrib_dir2_threshold_20\",\n",
    "    \"scr_attrib_dir2_threshold_50\",\n",
    "    \"scr_attrib_dir1_threshold_20\",\n",
    "    \"scr_attrib_dir1_threshold_50\",\n",
    "]\n",
    "\n",
    "metric_keys = [\n",
    "    \"l0\",\n",
    "    \"frac_recovered\",\n",
    "    \"tpp_attrib_threshold_50_total_metric\",\n",
    "    \"tpp_auto_interp_threshold_50_total_metric\",\n",
    "    \"scr_bias_shift_dir2_threshold_50\",\n",
    "    # \"scr_bias_shift_dir1_threshold_50\",\n",
    "    \"scr_attrib_dir2_threshold_50\",\n",
    "    # \"scr_attrib_dir1_threshold_50\",\n",
    "]\n",
    "\n",
    "plot_correlation_heatmap(plotting_results, metric_names=metric_keys, ae_names=None, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metric = f\"scr_bias_shift_dir1_threshold_{threshold}\"\n",
    "custom_metric = f\"scr_attrib_dir2_threshold_{threshold}\"\n",
    "\n",
    "title = f\"L0 vs Loss Recovered vs {custom_metric}\"\n",
    "\n",
    "plot_3var_graph(plotting_results, title, custom_metric)\n",
    "plot_2var_graph(plotting_results, custom_metric, title=title, y_label=\"Custom Metric\")\n",
    "plot_interactive_3var_graph(plotting_results, custom_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plotting_results[first_key].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = f\"scr_bias_shift_dir1_threshold_{threshold}\"\n",
    "metric2 = f\"scr_attrib_dir1_threshold_{threshold}\"\n",
    "title = f\"{metric1} vs {metric2}\"\n",
    "title = \"\"\n",
    "\n",
    "output_filename = os.path.join(image_folder_name, f\"scr_comparison_{threshold}_{model_name}.png\")\n",
    "\n",
    "plot_metric_scatter(\n",
    "    plotting_results,\n",
    "    metric_x=metric1,\n",
    "    metric_y=metric2,\n",
    "    title=title,\n",
    "    x_label=\"SHIFT with LLM Judge\",\n",
    "    y_label=\"SHIFT without LLM Judge\",\n",
    "    output_filename=output_filename,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1 = f\"tpp_auto_interp_threshold_{threshold}_total_metric\"\n",
    "metric2 = f\"scr_attrib_dir1_threshold_{threshold}\"\n",
    "title = f\"{metric1} vs {metric2}\"\n",
    "plot_metric_scatter(plotting_results, metric_x=metric1, metric_y=metric2, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
