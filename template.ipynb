{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will display the dataframe containing SAEBench releases and saes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                              release  \\\n",
      "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_...  sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2...   \n",
      "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_...  sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8...   \n",
      "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824       sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824   \n",
      "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824       sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824   \n",
      "sae_bench_pythia70m_sweep_gated_ctx128_0730               sae_bench_pythia70m_sweep_gated_ctx128_0730   \n",
      "\n",
      "                                                            repo_id  \\\n",
      "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_...  canrager/lm_sae   \n",
      "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_...  canrager/lm_sae   \n",
      "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824     canrager/lm_sae   \n",
      "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824     canrager/lm_sae   \n",
      "sae_bench_pythia70m_sweep_gated_ctx128_0730         canrager/lm_sae   \n",
      "\n",
      "                                                         model  \\\n",
      "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_...    gemma-2b   \n",
      "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_...    gemma-2b   \n",
      "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824       gemma-2b   \n",
      "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824       gemma-2b   \n",
      "sae_bench_pythia70m_sweep_gated_ctx128_0730         pythia-70m   \n",
      "\n",
      "                                                                                             saes_map  \\\n",
      "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_...  {'blocks.3.hook_resid_post__trainer_1_step_292...   \n",
      "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_...  {'blocks.3.hook_resid_post__trainer_5_step_488...   \n",
      "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824     {'blocks.3.hook_resid_post__trainer_2_step_976...   \n",
      "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824     {'blocks.3.hook_resid_post__trainer_3': 'gemma...   \n",
      "sae_bench_pythia70m_sweep_gated_ctx128_0730         {'blocks.3.hook_resid_post__trainer_0': 'pythi...   \n",
      "\n",
      "                                                                                       neuronpedia_id  \n",
      "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_...  {'blocks.3.hook_resid_post__trainer_1_step_292...  \n",
      "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_...  {'blocks.3.hook_resid_post__trainer_5_step_488...  \n",
      "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824     {'blocks.3.hook_resid_post__trainer_2_step_976...  \n",
      "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824     {'blocks.3.hook_resid_post__trainer_3': None, ...  \n",
      "sae_bench_pythia70m_sweep_gated_ctx128_0730         {'blocks.3.hook_resid_post__trainer_0': None, ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "\n",
    "# TODO: Make this nicer.\n",
    "df = pd.DataFrame.from_records(\n",
    "    {k: v.__dict__ for k, v in get_pretrained_saes_directory().items()}\n",
    ").T\n",
    "df.drop(\n",
    "    columns=[\"expected_var_explained\", \"expected_l0\", \"config_overrides\", \"conversion_func\"],\n",
    "    inplace=True,\n",
    ")\n",
    "filtered_df = df[\n",
    "    df.release.str.contains(\"bench\")\n",
    "]  # Each row is a \"release\" which has multiple SAEs which may have different configs / match different hook points in a model.\n",
    "\n",
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are 8 SAE Bench releases: 4 for Pythia and 4 for Gemma-2-2B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_0824    sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2...\n",
      "sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_0824    sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8...\n",
      "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824          sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824\n",
      "sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824          sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824\n",
      "sae_bench_pythia70m_sweep_gated_ctx128_0730                  sae_bench_pythia70m_sweep_gated_ctx128_0730\n",
      "sae_bench_pythia70m_sweep_panneal_ctx128_0730              sae_bench_pythia70m_sweep_panneal_ctx128_0730\n",
      "sae_bench_pythia70m_sweep_standard_ctx128_0712            sae_bench_pythia70m_sweep_standard_ctx128_0712\n",
      "sae_bench_pythia70m_sweep_topk_ctx128_0730                    sae_bench_pythia70m_sweep_topk_ctx128_0730\n",
      "Name: release, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(filtered_df.release)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each idea will contain a dict of sae_id: sae_name. The ids are used to load the SAEs into SAELens.\n",
    "\n",
    "We use the SAE names as keys in our results dictionaries, rather than the SAE ids. This is because the names are unique, and there's no possibility of mixing data between different SAEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sae id: blocks.3.hook_resid_post__trainer_3\n",
      "First sae name: gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_3/trainer_3\n"
     ]
    }
   ],
   "source": [
    "sae_release = \"sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824\"\n",
    "sae_id_to_name_map = filtered_df.saes_map[sae_release]\n",
    "sae_name_to_id_map = {v: k for k, v in sae_id_to_name_map.items()}\n",
    "\n",
    "print(f\"First sae id: {list(sae_id_to_name_map.keys())[0]}\")\n",
    "print(f\"First sae name: {list(sae_id_to_name_map.values())[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of loading a Pythia SAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "device = \"cpu\"\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"sae_bench_pythia70m_sweep_topk_ctx128_0730\",\n",
    "    sae_id=\"blocks.4.hook_resid_post__trainer_10\",\n",
    "    device=device,\n",
    ")\n",
    "sae = sae.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example input of `sae_names` and `sae_release` for the `sparse_probing` eval function input. I'm using the names, not ids, to match the convention of our results dictionaries, but you could also pass in SAE ids if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_release = \"sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824\"\n",
    "\n",
    "sae_names = [\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_19/trainer_0\",\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_19/trainer_1\",\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_19/trainer_2\",\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_19/trainer_3\",\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_19/trainer_4\",\n",
    "    \"gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_19/trainer_5\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already committed `sae_bench_data/{release_name}_data.json`. This contains the config used in the `dictionary_learning` repo, which includes training hyperparameters, SAE type, etc. It also contains the `basic_eval_results`, which includes the `l0` and `frac_recovered`, which was obtained using the `dictionary_learning evaluate()` function. These are already computed, so we can use them when making graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_name = \"sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824\"\n",
    "\n",
    "folder_path = \"sparse_probing/src/sparse_probing_results\"\n",
    "filename = f\"example_results_{release_name}_eval_results.json\"\n",
    "\n",
    "filepath = os.path.join(folder_path, filename)\n",
    "\n",
    "with open(filepath, \"r\") as f:\n",
    "    custom_eval_results = json.load(f)\n",
    "\n",
    "sae_data_filename = f\"sae_bench_data/{release_name}_data.json\"\n",
    "\n",
    "with open(sae_data_filename, \"r\") as f:\n",
    "    sae_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `sae_data` contains two keys: 'sae_config_dictionary_learning' and 'basic_eval_results'. Within each key, we have all SAE names for that release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sae_config_dictionary_learning', 'basic_eval_results'])\n",
      "['gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_19/trainer_4', 'gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_19/trainer_3', 'gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_19/trainer_2', 'gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_19/trainer_5', 'gemma-2-2b_sweep_topk_ctx128_ef8_0824/resid_post_layer_19/trainer_0']\n",
      "\n",
      " {'trainer': {'trainer_class': 'TrainerTopK', 'dict_class': 'AutoEncoderTopK', 'lr': 0.0001885618083164127, 'steps': 48828, 'seed': 0, 'activation_dim': 2304, 'dict_size': 18432, 'k': 320, 'device': 'cuda:0', 'layer': 19, 'lm_name': 'google/gemma-2-2b', 'wandb_name': 'TopKTrainer-google/gemma-2-2b-resid_post_layer_19', 'submodule_name': 'resid_post_layer_19'}, 'buffer': {'d_submodule': 2304, 'io': 'out', 'n_ctxs': 2000, 'ctx_len': 128, 'refresh_batch_size': 32, 'out_batch_size': 4096, 'device': 'cuda:0'}}\n"
     ]
    }
   ],
   "source": [
    "print(sae_data.keys())\n",
    "first_key = list(sae_data.keys())[0]\n",
    "print(list(sae_data[first_key].keys())[:5])\n",
    "first_sae_key = list(sae_data[first_key].keys())[0]\n",
    "print(\"\\n\", sae_data[first_key][first_sae_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our custom eval results should look like the following. It should contain a `custom_eval_config` key, which contains all hyperparameters and config values to reproduce the results.\n",
    "\n",
    "`custom_eval_results` contains a dict, where every key is an SAE name, and every value is another dict containing various results from the eval. This dict can be immediately loaded in to `graph_sae_results.ipynb` to create various plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['custom_eval_results', 'custom_eval_config'])\n",
      "{'random_seed': 42, 'model_dtype': 'torch.bfloat16', 'dataset_name': 'bias_in_bios', 'probe_train_set_size': 4000, 'probe_test_set_size': 1000, 'context_length': 128, 'probe_batch_size': 250, 'epochs': 10, 'lr': 0.001, 'sae_batch_size': 125, 'sae_release': 'sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824', 'model_name': 'gemma-2-2b'}\n",
      "{'llm_test_accuracy': 0.9652000427246094, 'sae_test_accuracy': 0.9644000411033631, 'sae_top_1_test_accuracy': 0.7892000436782837, 'sae_top_2_test_accuracy': 0.8430000305175781, 'sae_top_5_test_accuracy': 0.8790000557899476, 'sae_top_10_test_accuracy': 0.9036000370979309, 'sae_top_20_test_accuracy': 0.9196000456809997, 'sae_top_50_test_accuracy': 0.9492000460624694, 'sae_top_100_test_accuracy': 0.953600037097931}\n"
     ]
    }
   ],
   "source": [
    "print(custom_eval_results.keys())\n",
    "print(custom_eval_results[\"custom_eval_config\"])\n",
    "print(custom_eval_results[\"custom_eval_results\"][first_sae_key])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
