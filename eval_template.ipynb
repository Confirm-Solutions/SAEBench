{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Bench Eval Template\n",
    "\n",
    "## Overview\n",
    "\n",
    "Every eval type has the following:\n",
    "1. A corresponding sub-package. \n",
    "2. A main.py, which includes:\n",
    "   1.  An argparse interface (`arg_parse`) for running the eval from the command line \n",
    "   2.  A run_eval function which operates on a set of SAEs, producing a json with results per SAE. \n",
    "3. An eval_config.py with config specific to that eval and defaults set to recommended values. \n",
    "\n",
    "## CLI and Eval Config\n",
    "\n",
    "The CLI interface takes a combination of common arguments (same for all evals) and eval-type specific arguments. Eval-type specific arguments should match those in the eval_config of that sub-package. The common eval arguments should include:\n",
    "- `sae_regex_pattern` and `sae_block_pattern` used with regex to select SAEs from the SAE Lens library. \n",
    "- `output_folder` to place the output in. \n",
    "- `model_name` for loading a model from TransformerLens.\n",
    "\n",
    "To see which SAEs you can select via the regex arguments, use the SAE selection utils like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephbloom/miniforge3/envs/sae_bench_template/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────────────────────────────┬─────────────────────────────────────────────────────┬────────────────────────────────────────────────────────┬──────────┐\n",
      "│ model                               │ release                                             │ repo_id                                                │   n_saes │\n",
      "├─────────────────────────────────────┼─────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┼──────────┤\n",
      "│ gemma-2-27b                         │ gemma-scope-27b-pt-res                              │ google/gemma-scope-27b-pt-res                          │       18 │\n",
      "│ gemma-2-27b                         │ gemma-scope-27b-pt-res-canonical                    │ google/gemma-scope-27b-pt-res                          │        3 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-res                               │ google/gemma-scope-2b-pt-res                           │      310 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-res-canonical                     │ google/gemma-scope-2b-pt-res                           │       58 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-mlp                               │ google/gemma-scope-2b-pt-mlp                           │      260 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-mlp-canonical                     │ google/gemma-scope-2b-pt-mlp                           │       52 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-att                               │ google/gemma-scope-2b-pt-att                           │      260 │\n",
      "│ gemma-2-2b                          │ gemma-scope-2b-pt-att-canonical                     │ google/gemma-scope-2b-pt-att                           │       52 │\n",
      "│ gemma-2-2b                          │ sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_0824 │ canrager/lm_sae                                        │      180 │\n",
      "│ gemma-2-2b                          │ sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_0824 │ canrager/lm_sae                                        │      240 │\n",
      "│ gemma-2-2b                          │ sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824     │ canrager/lm_sae                                        │      180 │\n",
      "│ gemma-2-2b                          │ sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824     │ canrager/lm_sae                                        │      240 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-res                               │ google/gemma-scope-9b-pt-res                           │      562 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-res-canonical                     │ google/gemma-scope-9b-pt-res                           │       91 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-att                               │ google/gemma-scope-9b-pt-att                           │      492 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-att-canonical                     │ google/gemma-scope-9b-pt-att                           │       84 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-mlp                               │ google/gemma-scope-9b-pt-mlp                           │      492 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-pt-mlp-canonical                     │ google/gemma-scope-9b-pt-mlp                           │       84 │\n",
      "│ gemma-2-9b                          │ gemma-scope-9b-it-res                               │ google/gemma-scope-9b-it-res                           │       30 │\n",
      "│ gemma-2-9b-it                       │ gemma-scope-9b-it-res-canonical                     │ google/gemma-scope-9b-it-res                           │        6 │\n",
      "│ gemma-2b                            │ gemma-2b-res-jb                                     │ jbloom/Gemma-2b-Residual-Stream-SAEs                   │        5 │\n",
      "│ gemma-2b-it                         │ gemma-2b-it-res-jb                                  │ jbloom/Gemma-2b-IT-Residual-Stream-SAEs                │        1 │\n",
      "│ gpt2-small                          │ gpt2-small-res-jb                                   │ jbloom/GPT2-Small-SAEs-Reformatted                     │       13 │\n",
      "│ gpt2-small                          │ gpt2-small-hook-z-kk                                │ ckkissane/attn-saes-gpt2-small-all-layers              │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-mlp-tm                                   │ tommmcgrath/gpt2-small-mlp-out-saes                    │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-res-jb-feature-splitting                 │ jbloom/GPT2-Small-Feature-Splitting-Experiment-Layer-8 │        8 │\n",
      "│ gpt2-small                          │ gpt2-small-resid-post-v5-32k                        │ jbloom/GPT2-Small-OAI-v5-32k-resid-post-SAEs           │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-resid-post-v5-128k                       │ jbloom/GPT2-Small-OAI-v5-128k-resid-post-SAEs          │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-resid-mid-v5-32k                         │ jbloom/GPT2-Small-OAI-v5-32k-resid-mid-SAEs            │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-resid-mid-v5-128k                        │ jbloom/GPT2-Small-OAI-v5-128k-resid-mid-SAEs           │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-mlp-out-v5-32k                           │ jbloom/GPT2-Small-OAI-v5-32k-mlp-out-SAEs              │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-mlp-out-v5-128k                          │ jbloom/GPT2-Small-OAI-v5-128k-mlp-out-SAEs             │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-attn-out-v5-32k                          │ jbloom/GPT2-Small-OAI-v5-32k-attn-out-SAEs             │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-attn-out-v5-128k                         │ jbloom/GPT2-Small-OAI-v5-128k-attn-out-SAEs            │       12 │\n",
      "│ gpt2-small                          │ gpt2-small-res_sll-ajt                              │ neuronpedia/gpt2-small__res_sll-ajt                    │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_slefr-ajt                            │ neuronpedia/gpt2-small__res_slefr-ajt                  │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_scl-ajt                              │ neuronpedia/gpt2-small__res_scl-ajt                    │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_sle-ajt                              │ neuronpedia/gpt2-small__res_sle-ajt                    │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_sce-ajt                              │ neuronpedia/gpt2-small__res_sce-ajt                    │        3 │\n",
      "│ gpt2-small                          │ gpt2-small-res_scefr-ajt                            │ neuronpedia/gpt2-small__res_scefr-ajt                  │        3 │\n",
      "│ meta-llama/Meta-Llama-3-8B-Instruct │ llama-3-8b-it-res-jh                                │ Juliushanhanhan/llama-3-8b-it-res                      │        1 │\n",
      "│ mistral-7b                          │ mistral-7b-res-wg                                   │ JoshEngels/Mistral-7B-Residual-Stream-SAEs             │        3 │\n",
      "│ pythia-70m                          │ sae_bench_pythia70m_sweep_gated_ctx128_0730         │ canrager/lm_sae                                        │       40 │\n",
      "│ pythia-70m                          │ sae_bench_pythia70m_sweep_panneal_ctx128_0730       │ canrager/lm_sae                                        │       56 │\n",
      "│ pythia-70m                          │ sae_bench_pythia70m_sweep_standard_ctx128_0712      │ canrager/lm_sae                                        │       44 │\n",
      "│ pythia-70m                          │ sae_bench_pythia70m_sweep_topk_ctx128_0730          │ canrager/lm_sae                                        │       48 │\n",
      "│ pythia-70m-deduped                  │ pythia-70m-deduped-res-sm                           │ ctigges/pythia-70m-deduped__res-sm_processed           │        7 │\n",
      "│ pythia-70m-deduped                  │ pythia-70m-deduped-mlp-sm                           │ ctigges/pythia-70m-deduped__mlp-sm_processed           │        6 │\n",
      "│ pythia-70m-deduped                  │ pythia-70m-deduped-att-sm                           │ ctigges/pythia-70m-deduped__att-sm_processed           │        6 │\n",
      "└─────────────────────────────────────┴─────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┴──────────┘\n",
      "┌────────────────────────┬─────────────────────────────────────────────────────────────────────────┐\n",
      "│ Field                  │ Value                                                                   │\n",
      "├────────────────────────┼─────────────────────────────────────────────────────────────────────────┤\n",
      "│ release                │ 'gpt2-small-res-jb'                                                     │\n",
      "│ repo_id                │ 'jbloom/GPT2-Small-SAEs-Reformatted'                                    │\n",
      "│ model                  │ 'gpt2-small'                                                            │\n",
      "│ conversion_func        │ None                                                                    │\n",
      "│ saes_map               │ {'blocks.0.hook_resid_pre': 'blocks.0.hook_resid_pre', ...}             │\n",
      "│ expected_var_explained │ {'blocks.0.hook_resid_pre': 0.999, ...}                                 │\n",
      "│ expected_l0            │ {'blocks.0.hook_resid_pre': 10.0, ...}                                  │\n",
      "│ neuronpedia_id         │ {'blocks.0.hook_resid_pre': 'gpt2-small/0-res-jb', ...}                 │\n",
      "│ config_overrides       │ {'model_from_pretrained_kwargs': {'center_writing_weights': True}, ...} │\n",
      "└────────────────────────┴─────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "from sae_bench_utils.sae_selection_utils import print_all_sae_releases, print_release_details\n",
    "\n",
    "# Callum came up with this format which I like visually.\n",
    "print_all_sae_releases() # each release has a corresponding model / repo_id. We recommend you don't select releases with different models when running evals.\n",
    "print_release_details('gpt2-small-res-jb') # each release has a number of possible SAEs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example call to the absorption eval. Note that we are selecting just one release / SAE (though we could select more) and that we're using default arguments for the eval-specific args (by not setting them via the CLI.)\n",
    "\n",
    "```bash\n",
    "python evals/absorption/main.py \\\n",
    "--sae_regex_pattern \"sae_bench_pythia70m_sweep_standard_ctx128_0712\" \\\n",
    "--sae_block_pattern \"blocks.4.hook_resid_post__trainer_.*\" \\\n",
    "--model_name pythia-70m-deduped \\\n",
    "--output_folder results\n",
    "```\n",
    "\n",
    "To create such an interface, an arg_parse function should be created in the main.py file as below and an EvalConfig should be instantiated in an eval_config.py file inside the eval subpackage. Eval configs should be dataclass objects that have serializable values (so it's easy to save them / load them.)\n",
    "\n",
    "You can test whether you've set up the `EvalConfig` and `arg_parser` correctly by using the `validate_eval_cli_interface` testing util. Feel free to change the CLI args / Eval Config to test the validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from sae_bench_utils.testing_utils import validate_eval_cli_interface\n",
    "\n",
    "def arg_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"Run absorption evaluation\")\n",
    "    parser.add_argument(\"--arg1\", type=int, default=42, help=\"Description for arg1\")\n",
    "    parser.add_argument(\"--arg2\", type=float, default=0.03, help=\"Description for arg2\")\n",
    "    parser.add_argument(\"--arg3\", type=int, default=10, help=\"Description for arg3\")\n",
    "    parser.add_argument(\"--arg4\", type=str, default=\"{word} has the first letter:\", help=\"Description for arg4\")\n",
    "    parser.add_argument(\"--arg5\", type=int, default=-6, help=\"Description for arg5\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"pythia-70m-deduped\", help=\"Description for arg6\")\n",
    "    parser.add_argument(\"--sae_regex_pattern\", type=str, required=True, help=\"Regex pattern for SAE selection\")\n",
    "    parser.add_argument(\"--sae_block_pattern\", type=str, required=True, help=\"Regex pattern for SAE block selection\")\n",
    "    parser.add_argument(\"--output_folder\", type=str, default=\"evals/absorption/results\", help=\"Output folder\")\n",
    "    parser.add_argument(\"--force_rerun\", action=\"store_true\", help=\"Force rerun of experiments\")\n",
    "\n",
    "    return parser\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    arg1: int = 42\n",
    "    arg2: float = 0.03\n",
    "    arg3: int = 10\n",
    "    arg4: str = \"{word} has the first letter:\"\n",
    "    arg5: int = -6\n",
    "    model_name: str = \"pythia-70m-deduped\"\n",
    "\n",
    "validate_eval_cli_interface(arg_parser(), eval_config_cls=EvalConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Output Format\n",
    "\n",
    "Each output json should correspond to one SAE and have the following fields:\n",
    "\n",
    "See an example here:\n",
    "- `eval_instance_id`: A unique UUID identifying this specific eval run\n",
    "- `sae_lens_release`: The release identifier of the SAE from SAE Lens\n",
    "- `sae_lens_id`: The specific identifier of the SAE within the release\n",
    "- `eval_type_id`: The type of evaluation being performed\n",
    "- `sae_lens_version`: Version of SAE Lens used\n",
    "- `sae_bench_version`: Version of SAE Bench used\n",
    "- `date_time`: Timestamp when the eval was run\n",
    "- `eval_config`: Object containing all configuration parameters used for this eval.\n",
    "- `eval_results`: Object containing the results of the evaluation.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"eval_instance_id\": \"0c057d5e-973e-410e-8e32-32569323b5e6\",\n",
    "    \"sae_lens_release\": \"sae_bench_pythia70m_sweep_standard_ctx128_0712\",\n",
    "    \"sae_lens_id\": \"blocks.3.hook_resid_post__trainer_10\",\n",
    "    \"eval_type_id\": \"absorption\",\n",
    "    \"sae_lens_version\": \"4.0.0\",\n",
    "    \"sae_bench_version\": \"57e9be0ac9199dba6b9f87fe92f80532e9aefced\",\n",
    "    \"date_time\": \"2024-10-22T10:46:36.799610\",\n",
    "    \"eval_config\": {\n",
    "        \"random_seed\": 42,\n",
    "        \"f1_jump_threshold\": 0.03,\n",
    "        \"max_k_value\": 10,\n",
    "        \"prompt_template\": \"{word} has the first letter:\",\n",
    "        \"prompt_token_pos\": -6,\n",
    "        \"model_name\": \"pythia-70m-deduped\"\n",
    "    },\n",
    "    \"eval_results\": {...},\n",
    "}\n",
    "```\n",
    "\n",
    "In order to validate an output json, you should use the `validate_eval_output_format` function from our testing utils.  Feel free to break the json and see the test fail. (eg: remove a field like `sae_lens_release`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from sae_bench_utils.testing_utils import validate_eval_output_format\n",
    "\n",
    "eval_results_temp = {\n",
    "    \"eval_instance_id\": \"0c057d5e-973e-410e-8e32-32569323b5e6\",\n",
    "    \"sae_lens_release\": \"sae_bench_pythia70m_sweep_standard_ctx128_0712\",\n",
    "    \"sae_lens_id\": \"blocks.3.hook_resid_post__trainer_10\",\n",
    "    \"eval_type_id\": \"absorption\",\n",
    "    \"sae_lens_version\": \"4.0.0\",\n",
    "    \"sae_bench_version\": \"57e9be0ac9199dba6b9f87fe92f80532e9aefced\",\n",
    "    \"date_time\": \"2024-10-22T10:46:36.799610\",\n",
    "    \"eval_config\": {\n",
    "        \"random_seed\": 42,\n",
    "        \"f1_jump_threshold\": 0.03,\n",
    "        \"max_k_value\": 10,\n",
    "        \"prompt_template\": \"{word} has the first letter:\",\n",
    "        \"prompt_token_pos\": -6,\n",
    "        \"model_name\": \"pythia-70m-deduped\"\n",
    "    },\n",
    "    \"eval_results\": {},\n",
    "}\n",
    "\n",
    "\n",
    "# save to file\n",
    "with open('eval_results_temp.json', 'w') as f:\n",
    "    json.dump(eval_results_temp, f)\n",
    "\n",
    "validate_eval_output_format('eval_results_temp.json', eval_type=\"absorption\")\n",
    "\n",
    "# delete file\n",
    "os.remove('eval_results_temp.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load the eval results jsons accross many different SAEs, have a high level of visibility into which evals were run with which parameters and code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## A note on cached results\n",
    "\n",
    "A variety of evals can share the results intermediate computation, such as model activations or trained probes. Most of these will be model / hook point specific so should be saved along a path of the format `f'{artifact_dir}/{eval_type}/{model}/{hook_point}/{artifact_id}'`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae_bench_template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
